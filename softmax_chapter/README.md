#### 一、模型原理

##### 1. 核心作用
Softmax分类器是解决多分类问题的经典模型，能将输入特征映射到多个类别的概率分布，最终选择概率最高的类别作为预测结果。


##### 2. 数学原理
假设输入特征为 $X \in \mathbb{R}^{n \times d}$（$n$ 个样本，$d$ 个特征），模型参数为权重矩阵 $W \in \mathbb{R}^{d \times c}$（$c$ 个类别）和偏置 $b \in \mathbb{R}^{c}$，则模型的计算过程分为三步：

1. **线性变换**  
   先计算每个类别的“原始分数”（logits）：  
   $$z = XW + b$$  
   其中 $z \in \mathbb{R}^{n \times c}$，每个元素 $z_{i,j}$ 表示第 $i$ 个样本属于第 $j$ 类的原始分数。

2. **Softmax函数**  
   将原始分数转换为概率分布（所有类别概率和为1）：  
   $$\sigma(z)_{i,j} = \frac{e^{z_{i,j} - \max(z_i)}}{\sum_{k=1}^c e^{z_{i,k} - \max(z_i)}}$$  
   - 分子：对原始分数做指数变换（保证概率非负），减去每行最大值是为了**数值稳定性**（避免指数溢出）。  
   - 分母：归一化处理，确保输出为合法概率分布。

3. **交叉熵损失**  
   用交叉熵衡量预测概率与真实标签的差距（真实标签用独热编码表示）：  
   $$L = -\frac{1}{n} \sum_{i=1}^n \sum_{j=1}^c y_{i,j} \log(\sigma(z)_{i,j}) + \frac{\lambda}{2} \|W\|_2^2$$  
   - 第一项：交叉熵损失，仅惩罚正确类别的预测概率（$y_{i,j}=1$ 时才计算）。  
   - 第二项：L2正则化，防止权重过大导致过拟合，$\lambda$ 为正则化系数。


##### 3. 直观理解
- **原始分数**：可理解为模型对每个类别的“偏好程度”，分数越高表示模型越倾向于该类别。  
- **Softmax函数**：类似“概率归一化器”，将偏好程度转换为可解释的概率（例如，3个类别的分数为 $[2, 1, 0]$，Softmax后概率约为 $[0.665, 0.244, 0.091]$）。  
- **训练目标**：通过调整 $W$ 和 $b$，使正确类别的概率尽可能接近1，错误类别的概率接近0。


#### 二、代码实现解析


##### 1. 初始化（`__init__`方法）
该方法用于存储模型的超参数，包括学习率（梯度下降的步长）、迭代次数（训练的总步数）、正则化系数（控制过拟合的强度）。同时，预留权重矩阵和偏置的存储空间（初始为未定义状态，将在训练时根据输入数据的维度确定具体形状），并创建一个列表用于记录训练过程中的损失变化，方便后续观察模型收敛情况。


##### 2. Softmax函数实现（`_softmax`方法）
该方法严格遵循数学公式实现。首先对原始分数进行数值稳定化处理：减去每个样本对应的所有类别分数中的最大值，避免指数运算时因数值过大导致的溢出。然后对处理后的分数进行指数变换，最后通过除以每行（每个样本）的指数和进行归一化，确保输出的每个样本对应的类别概率之和为1，符合概率分布的基本要求。


##### 3. 损失计算（`_compute_loss`方法）
该方法计算模型的总损失，分为两部分：交叉熵损失和正则化损失。  
- 交叉熵损失：先通过线性变换得到原始分数，再经Softmax函数转换为概率分布。然后提取每个样本对应正确类别的概率，对其取对数并取负（因交叉熵公式中包含负号），最后求所有样本的平均值，得到数据损失。为避免概率为0时对数无意义，会在概率值中加入一个极小的常数（如1e-8）。  
- 正则化损失：按照L2正则化公式，计算权重矩阵元素平方和的一半与正则化系数的乘积，用于约束权重大小，防止过拟合。  
总损失为交叉熵损失与正则化损失之和。


##### 4. 梯度计算（`_compute_gradients`方法）
该方法通过反向传播推导参数的梯度，是模型训练的核心：  
- 首先计算原始分数的梯度：将Softmax输出的概率分布中正确类别的概率减去1（对应独热编码中真实标签的“1”），再除以样本数量进行归一化，得到原始分数对损失的梯度。  
- 权重矩阵的梯度：由两部分组成，一部分是输入特征矩阵的转置与原始分数梯度的乘积（反映特征对损失的影响），另一部分是正则化项对权重的梯度（即正则化系数与权重矩阵的乘积）。  
- 偏置的梯度：直接对原始分数的梯度按样本维度求和（因偏置在线性变换中是直接相加的，其梯度不受特征影响）。


##### 5. 模型训练（`fit`方法）
该方法通过批量梯度下降优化参数：  
- 首先根据输入数据的特征数和类别数初始化参数：权重矩阵采用小随机数（避免初始权重对称导致的学习效率低下），偏置初始化为0。  
- 然后迭代指定次数，每次迭代中先计算权重和偏置的梯度，再根据梯度下降公式更新参数（参数 = 参数 - 学习率 × 梯度）。  
- 为了监控训练过程，每间隔一定迭代次数（如100次）计算一次当前损失并记录，方便观察模型是否收敛。


##### 6. 预测相关方法（`predict_proba`、`predict`、`score`方法）
- `predict_proba`：利用训练好的权重和偏置，对输入特征进行线性变换得到原始分数，再通过Softmax函数转换为每个类别的概率分布，输出概率结果。  
- `predict`：在`predict_proba`输出的概率基础上，对每个样本选择概率最大的类别作为预测结果。  
- `score`：计算预测正确的样本数占总样本数的比例（准确率），用于衡量模型在给定数据上的表现。


Softmax分类器通过线性变换+Softmax归一化实现多分类，核心是用交叉熵损失衡量概率分布差异，并通过梯度下降优化参数。代码实现中，各方法严格对应数学原理，同时加入数值稳定性处理（如减去最大值、加极小常数）和正则化机制，确保模型训练稳定且具有良好的泛化能力。
#### 三、实验结果与分析


##### 1、实验内容：
- **数据集**：自定义数据集（1000个样本，20个特征，5个类别），按8:2划分为训练集和测试集。
- **模型**：超参数为学习率0.01、迭代次数1000、正则化系数0.01。
- **训练目标**：通过多分类交叉熵损失+L2正则化优化，实现对5个类别的准确预测。


##### 2、实验结果

- 迭代初期（0-200次）：损失从0.0460快速下降至0.0119，表明模型快速学习到数据模式。
- 迭代中期（200-500次）：损失下降趋缓，逐渐稳定在0.0115左右。
- 迭代后期（500-1000次）：损失几乎不变，说明模型已收敛。

- 训练集准确率：1.0000（所有训练样本预测正确）。
- 测试集准确率：1.0000（所有测试样本预测正确）。

##### 3、结果分析

- 损失曲线在500次迭代后稳定，说明模型通过梯度下降有效优化了参数，且超参数（学习率、迭代次数）设置合理：学习率既保证了收敛速度，又避免了震荡；迭代次数足够使模型达到稳定状态。
- 训练集与测试集准确率均为100%，表明模型在该数据集上拟合效果极佳，且无过拟合现象。
- 数据集特征与标签的映射关系明确，线性可分性强。L2正则化有效约束了权重规模，避免了过拟合。
##### 4、结论
- 正确类别的预测概率接近1，错误类别概率接近0，符合Softmax函数的设计目标：通过归一化将“偏好分数”转化为置信度高的概率分布。这说明模型对每个样本的类别判断具有强确定性，而非随机猜测。验证了Softmax分类器在多分类任务中的有效性：通过线性变换+交叉熵损失，可实现对简单数据集的高精度分类。
##### 5、局限
实验数据集可能过于简单（线性可分），而实际场景中的数据往往具有非线性关系，此时纯线性的Softmax分类器性能会下降（需结合特征工程或深度学习模型）。


#### 四、优化
##### 1、核心局限
原始版本依赖批量梯度下降（BGD），存在多方面不足：固定迭代次数导致冗余计算，即使损失损失稳定损失稳定稳定后仍持续迭代；对复杂数据泛化能力弱，在类别重叠、高噪声场景中过拟合风险高；仅支持全量数据更新，面对大数据集时易出现内存瓶颈，且单一优化器难以适配非线性边界数据，在高难度任务中准确率受限。

##### 2、优化内容
优化版本通过多重改进解决原始问题：引入Momentum和Adam两种优化器，前者借助动量加速收敛，后者结合自适应学习率应对复杂数据；加入早停机制，依据验证集性能自动终止训练，减少无效迭代并抑制过拟合；支持Mini-batch训练，将全量数据拆分后更新，既解决大数据集内存问题，又通过随机性提升泛化能力。

##### 3、优化效果
优化版本在多场景中表现更优：超高难度高噪声场景下，Adam优化器测试准确率较原始版本提升1.5%，过拟合程度显著降低；中等难度场景中，Momentum优化器收敛轮次减少45%，过拟合程度下降22%；尽管部分场景训练时间略有增加，但整体在准确率、收敛效率和泛化能力上实现了平衡，尤其在复杂数据中优势明显，具体优化结果可查看comparison.txt文件。

##### 4、适用场景
优化版本更适合复杂数据场景，包括类别重叠、高噪声（噪声≥0.5）、非线性边界的数据；也适用于大数据集（样本量>2000、特征数>40），能通过Mini-batch机制突破内存限制；对于泛化需求高的任务（如医疗、金融数据），早停和随机性设计可有效提升模型稳健性。

##### 5、场景对比

原始版本适用于简单数据（线性可分、低噪声）和小数据集（样本量<2000），代码简洁且训练速度快，准确率与优化版本持平；优化版本在复杂、高噪声、大规模数据中更具优势，Adam在高难度场景准确率突出，Momentum则在中等复杂度数据上平衡了速度与泛化能力，适合实际业务中多样化的任务需求。
